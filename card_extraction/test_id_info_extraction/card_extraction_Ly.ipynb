{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from maskrcnn import MaskrcnnResnet50FPN\n",
    "\n",
    "num_classes = 3\n",
    "weight_path = './weight/2011110823/best_model_31_dice_mAP=0.9705.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskrcnnResnet50FPN(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(weight_path, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskrcnnResnet50FPN(\n",
       "  (model): MaskRCNN(\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "        Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "    )\n",
       "    (backbone): BackboneWithFPN(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fpn): FeaturePyramidNetwork(\n",
       "        (inner_blocks): ModuleList(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (layer_blocks): ModuleList(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (extra_blocks): LastLevelMaxPool()\n",
       "      )\n",
       "    )\n",
       "    (rpn): RegionProposalNetwork(\n",
       "      (anchor_generator): AnchorGenerator()\n",
       "      (head): RPNHead(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): RoIHeads(\n",
       "      (box_roi_pool): MultiScaleRoIAlign()\n",
       "      (box_head): TwoMLPHead(\n",
       "        (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (box_predictor): FastRCNNPredictor(\n",
       "        (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "      )\n",
       "      (mask_roi_pool): MultiScaleRoIAlign()\n",
       "      (mask_head): MaskRCNNHeads(\n",
       "        (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu3): ReLU(inplace=True)\n",
       "        (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (relu4): ReLU(inplace=True)\n",
       "      )\n",
       "      (mask_predictor): MaskRCNNPredictor(\n",
       "        (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (mask_fcn_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image_size = (768, 768)\n",
    "\n",
    "def preprocess(image):\n",
    "    sample = cv2.resize(image, dsize=image_size)\n",
    "    sample = torch.from_numpy(sample).to(torch.float).to(device)\n",
    "    samples = sample.unsqueeze(dim=0).permute(0, 3, 1, 2)\n",
    "    samples = (samples - samples.mean()) / samples.std() #Tùy thuộc vào lúc train norm cách nào\n",
    "    return image, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image, samples):\n",
    "    with torch.no_grad():\n",
    "        preds = model(samples)\n",
    "    return image, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from shapely.geometry import box, Point, Polygon\n",
    "\n",
    "\n",
    "class EnclosingQuadrilateral:\n",
    "    def __init__(self):\n",
    "        self.binary_threshold = 0.6\n",
    "        self.area_threshold = 0.0\n",
    "        self.vertical_threshold = 20\n",
    "        self.iou_threshold = 0.8\n",
    "\n",
    "    def _order_points(self, points):\n",
    "        assert len(points) == 4, 'Length of points must be 4'\n",
    "        left = sorted(points, key=lambda p: p[0])[:2]\n",
    "        right = sorted(points, key=lambda p: p[0])[2:]\n",
    "        tl, bl = sorted(left, key=lambda p: p[1])\n",
    "        tr, br = sorted(right, key=lambda p: p[1])\n",
    "        return [tl, tr, br, bl]\n",
    "\n",
    "    def _compute_iou(self, polyA, polyB):\n",
    "        iou = 0.\n",
    "        polyA = Polygon(polyA)\n",
    "        polyB = Polygon(polyB)\n",
    "        if polyA.intersects(polyB):\n",
    "            iou = polyA.intersection(polyB).area / polyA.union(polyB).area\n",
    "        return iou\n",
    "\n",
    "    def _intersection_point(self, line1, line2):\n",
    "        a1 = line1[1][1] - line1[0][1]\n",
    "        b1 = line1[0][0] - line1[1][0]\n",
    "        a2 = line2[1][1] - line2[0][1]\n",
    "        b2 = line2[0][0] - line2[1][0]\n",
    "        determinant = a1 * b2 - a2 * b1\n",
    "        if determinant == 0:\n",
    "            return None\n",
    "        c1 = (a1 / determinant) * line1[0][0] + (b1 / determinant) * line1[0][1]\n",
    "        c2 = (a2 / determinant) * line2[0][0] + (b2 / determinant) * line2[0][1]\n",
    "        x = b2 * c1 - b1 * c2\n",
    "        y = a1 * c2 - a2 * c1\n",
    "        return [int(x), int(y)]\n",
    "\n",
    "    def _convex_hulls(self, pred, binary_threshold=0.6, area_threshold=0.0, vertical_threshold=20):\n",
    "        convex_hulls = []\n",
    "        binary_image = (pred > binary_threshold).astype(np.uint8)\n",
    "        binary_image = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, np.ones(shape=(5, 5), dtype=np.uint8))\n",
    "        num_label, label = cv2.connectedComponents(binary_image)\n",
    "        for i in range(1, num_label):\n",
    "            contours, _ = cv2.findContours((label == i).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contour = contours[0]\n",
    "            if cv2.contourArea(contour) > area_threshold * pred.size:\n",
    "                epsilon = 0.009 * cv2.arcLength(contour, closed=True)\n",
    "                approx_contour = cv2.approxPolyDP(contour, epsilon, closed=True)\n",
    "                convex_hull = cv2.convexHull(approx_contour)  # approximate contour to reduce num of points\n",
    "                for inc in range(5):\n",
    "                    if convex_hull.shape[0] <= vertical_threshold:\n",
    "                        break\n",
    "                    epsilon = 0.002 * (1 + inc) * cv2.arcLength(contour, closed=True)\n",
    "                    convex_hull = cv2.approxPolyDP(convex_hull, epsilon, closed=True)\n",
    "\n",
    "                if 4 <= convex_hull.shape[0] <= vertical_threshold:\n",
    "                    convex_hulls.append(np.squeeze(np.array(convex_hull), axis=1))\n",
    "\n",
    "        return convex_hulls\n",
    "\n",
    "    def _enclosing_quadrilateral(self, pred, convex_hulls, iou_threshold):\n",
    "        enclosing_quads = []\n",
    "        x1, x2 = [-pred.shape[0], 2 * pred.shape[0]]\n",
    "        y1, y2 = [-pred.shape[1], 2 * pred.shape[1]]\n",
    "        boundary = box(x1, y1, x2, y2)\n",
    "        for polygon in convex_hulls:\n",
    "            num_verticals = len(polygon)\n",
    "            max_iou = 0.\n",
    "            enclosing_quad = None\n",
    "            for (x, y, z, t) in itertools.combinations(range(num_verticals), 4):\n",
    "                lines = [\n",
    "                    [polygon[x], polygon[(x + 1) % num_verticals]],\n",
    "                    [polygon[y], polygon[(y + 1) % num_verticals]],\n",
    "                    [polygon[z], polygon[(z + 1) % num_verticals]],\n",
    "                    [polygon[t], polygon[(t + 1) % num_verticals]]\n",
    "                ]\n",
    "                points = []\n",
    "                for i in range(4):\n",
    "                    point = self._intersection_point(lines[i], lines[(i + 1) % 4])\n",
    "                    if (not point) or (point in points) or (not boundary.contains(Point(point))):\n",
    "                        break\n",
    "                    points.append(point)\n",
    "\n",
    "                if len(points) == 4 and Polygon(self._order_points(points)).is_valid:\n",
    "                    candidate_quad = self._order_points(points)\n",
    "                    iou = self._compute_iou(candidate_quad, polygon)\n",
    "                    if iou > max_iou and iou > iou_threshold:\n",
    "                        enclosing_quad = candidate_quad\n",
    "                        max_iou = iou\n",
    "\n",
    "            if enclosing_quad:\n",
    "                enclosing_quads.append(enclosing_quad)\n",
    "\n",
    "        return enclosing_quads\n",
    "    \n",
    "    def perspective_transform(self, original_image, pred_mask, quad):\n",
    "        width_ratio = original_image.shape[1] / pred_mask.shape[1]\n",
    "        height_ratio = original_image.shape[0] / pred_mask.shape[0] \n",
    "        quad = np.array(quad, dtype=np.float32)\n",
    "        quad[:, 0] = quad[:, 0] * width_ratio\n",
    "        quad[:, 1] = quad[:, 1] * height_ratio\n",
    "        tl, tr, br, bl = quad\n",
    "        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "        maxWidth = max(int(widthA), int(widthB))\n",
    "        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "        maxHeight = max(int(heightA), int(heightB))\n",
    "        dst = np.array([\n",
    "            [0, 0],\n",
    "            [maxWidth - 1, 0],\n",
    "            [maxWidth - 1, maxHeight - 1],\n",
    "            [0, maxHeight - 1]], dtype = \"float32\")\n",
    "        M = cv2.getPerspectiveTransform(quad, dst)\n",
    "        warped_image = cv2.warpPerspective(original_image, M, (maxWidth, maxHeight))\n",
    "        return warped_image, [tl, tr, br, bl]\n",
    "\n",
    "    def __call__(self, original_image, pred_mask):\n",
    "        warped_images = []\n",
    "        convex_hulls = self._convex_hulls(pred_mask,\n",
    "                                          self.binary_threshold,\n",
    "                                          self.area_threshold,\n",
    "                                          self.vertical_threshold)\n",
    "        enclosing_quads = self._enclosing_quadrilateral(pred_mask,\n",
    "                                                        convex_hulls,\n",
    "                                                        self.iou_threshold)\n",
    "        for quad in enclosing_quads:\n",
    "            warped_images.append(self.perspective_transform(original_image, pred_mask, quad))\n",
    "\n",
    "        return warped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(image, preds):\n",
    "    pred = preds[0]\n",
    "    boxes = pred['boxes']\n",
    "    masks = pred['masks']\n",
    "    scores = pred['scores']\n",
    "    labels = pred['labels']\n",
    "    \n",
    "    indices = scores > 0.5\n",
    "    masks = masks[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    indices = torchvision.ops.nms(boxes, scores, 0.5)\n",
    "    masks = masks[indices]\n",
    "    scores = scores[indices]\n",
    "    labels = labels[indices]  \n",
    "    \n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    masks = masks.squeeze(1).detach().cpu().numpy()\n",
    "    \n",
    "    warped_images = []\n",
    "    min_enclosing_quad = EnclosingQuadrilateral()\n",
    "    for mask in masks:\n",
    "        warped_image = min_enclosing_quad(image, mask)\n",
    "        warped_images += warped_image\n",
    "    \n",
    "    return warped_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Closing Quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = cv2.imread('./test/mask.jpg', cv2.IMREAD_GRAYSCALE) / 255.\n",
    "\n",
    "enclosing_quad = EnclosingQuadrilateral()\n",
    "convex_hulls = enclosing_quad._convex_hulls(pred_mask, 0.6, 0, 20)\n",
    "enclosing_quads = enclosing_quad._enclosing_quadrilateral(pred_mask,\n",
    "                                                convex_hulls,\n",
    "                                                0.8)\n",
    "\n",
    "for quad in enclosing_quads:\n",
    "    quad = np.int32(quad)\n",
    "    test_quad = np.stack([pred_mask] * 3, axis=2)\n",
    "    cv2.polylines(test_quad, [quad], True, (255, 0, 0), 3)\n",
    "    cv2.imshow('find quad', test_quad)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinhloiit/.local/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "test_image = cv2.imread('test/GiayCMND.png')\n",
    "image, samples = preprocess(test_image)\n",
    "image, preds = process(image, samples)\n",
    "warped_images = postprocess(image, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b587fa4f2119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwarped_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwarped_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarped_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'mat'"
     ]
    }
   ],
   "source": [
    "for warped_image in warped_images:\n",
    "    cv2.imshow('result', warped_image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from maskrcnn import MaskrcnnResnet50FPN\n",
    "\n",
    "\n",
    "class CardExtraction:\n",
    "    def __init__(self, num_classes, weight_path, image_size):\n",
    "        self.image_size = image_size\n",
    "        self.model = MaskrcnnResnet50FPN(num_classes=num_classes)\n",
    "        self.model.load_state_dict(torch.load(weight_path, map_location='cpu'))\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        sample = cv2.resize(image, dsize=self.image_size)\n",
    "        sample = torch.from_numpy(sample).to(torch.float).to(self.device)\n",
    "        samples = sample.unsqueeze(dim=0).permute(0, 3, 1, 2)\n",
    "        samples = (samples - samples.mean()) / samples.std() \n",
    "        return image, samples\n",
    "    \n",
    "    def process(self, image, samples):\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(samples)\n",
    "        return image, preds\n",
    "            \n",
    "    def postprocess(self, image, preds):\n",
    "        pred = preds[0]\n",
    "        boxes = pred['boxes']\n",
    "        masks = pred['masks']\n",
    "        scores = pred['scores']\n",
    "        labels = pred['labels']\n",
    "\n",
    "        indices = torchvision.ops.nms(boxes, scores, 0.5)\n",
    "        masks = masks[indices]\n",
    "        scores = scores[indices]\n",
    "        labels = labels[indices]  \n",
    "        \n",
    "        indices = scores > 0.5\n",
    "        masks = masks[indices]\n",
    "        labels = labels[indices]\n",
    "\n",
    "\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        masks = masks.squeeze(1).detach().cpu().numpy()\n",
    "\n",
    "        warped_images = []\n",
    "        min_enclosing_quad = EnclosingQuadrilateral()\n",
    "        for mask in masks:\n",
    "            warped_image = min_enclosing_quad(image, mask)\n",
    "            warped_images += warped_image\n",
    "\n",
    "        return warped_images\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        output = self.preprocess(*args)\n",
    "        output = self.process(*output)\n",
    "        output = self.postprocess(*output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('test/GiayCMND.png')\n",
    "card_extractor = CardExtraction(3, \n",
    "                                './weight/2011110823/best_model_31_dice_mAP=0.9705.pt', \n",
    "                                image_size=(768, 768))\n",
    "warped_images = card_extractor(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for warped_image in warped_images:\n",
    "    cv2.imshow('result', warped_image[0])\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    image_patterns = ['*.jpg', '*.png', '*.jpeg', '*.JPG', '*.PNG', '*.JPEG']\n",
    "    input_dir = Path('./test/test/')\n",
    "    image_paths = []\n",
    "    for image_pattern in image_patterns:\n",
    "        image_paths += list(input_dir.glob(f'**/{image_pattern}'))\n",
    "        \n",
    "    card_extractor = CardExtraction(3, \n",
    "                                './weight/2011110823/best_model_31_dice_mAP=0.9705.pt', \n",
    "                                image_size=(768, 768))\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(str(image_path))\n",
    "        warped_images = card_extractor(image)\n",
    "        for i in range(len(warped_images)):\n",
    "            cv2.imwrite(f'./test/test/output/{image_path.stem}_{i}.jpg', warped_images[i][0])\n",
    "            pts = np.int32(warped_images[i][1])\n",
    "            cv2.polylines(image, [pts], True, (0, 255, 0), 2) \n",
    "            cv2.imwrite(f'./test/test/output/{image_path.stem}_box_{i}.jpg', image)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
